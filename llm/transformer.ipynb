{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://img.shields.io/badge/language-cb2888) Train Transformer\n",
    "\n",
    "In this guide,  a simple transformer model is trained from scratch to perform machine translation as detailed in [Attention is all you need](https://arxiv.org/abs/1706.03762) from Arabic to English. But you are free to choose any language pair from the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ASEM000/serket --quiet\n",
    "!pip install tokenizers --quiet\n",
    "!pip install datasets --quiet\n",
    "!pip install ml_collections --quiet\n",
    "!pip install tqdm --quiet\n",
    "!pip install more-itertools --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerics related\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "import serket as sk\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "# dataset related\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from more_itertools import chunked\n",
    "\n",
    "# config related\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "# typing related\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from typing import TypeVar, Generic\n",
    "\n",
    "# other\n",
    "import functools as ft\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class Batched(Generic[T]):\n",
    "    # a type marker to indicate batch dimension\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigDict()\n",
    "\n",
    "# dataset\n",
    "config.dataset = ConfigDict()\n",
    "config.dataset.name = \"opus100\"\n",
    "config.dataset.src_lang = \"ar\"\n",
    "config.dataset.tgt_lang = \"en\"\n",
    "\n",
    "\n",
    "config.tokenizer = ConfigDict()\n",
    "config.tokenizer.unk_token = \"[UNK]\"\n",
    "config.tokenizer.special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"]\n",
    "config.tokenizer.min_frequency = 2\n",
    "\n",
    "config.model = ConfigDict()\n",
    "config.model.d_model = 512\n",
    "config.model.seq_len = 100\n",
    "config.model.num_heads = 8\n",
    "config.model.num_blocks = 2\n",
    "config.model.drop_rate = 0.1\n",
    "config.model.seed = 0\n",
    "\n",
    "config.train = ConfigDict()\n",
    "config.train.epochs = 1\n",
    "config.train.seed = 1\n",
    "config.train.batch_size = 32\n",
    "\n",
    "config.optim = ConfigDict()\n",
    "config.optim.lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_init = jax.nn.initializers.normal()\n",
    "\n",
    "\n",
    "class InputEmbedding(sk.TreeClass):\n",
    "    \"\"\"Lookup table for input tokens\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, d_model: int, *, key: jax.Array):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embed = embed_init(key, (vocab_size, d_model))\n",
    "\n",
    "    def __call__(self, input: jax.Array):\n",
    "        return jnp.take(self.embed, input, axis=0)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(sk.TreeClass):\n",
    "    def __init__(self, d_model: int, seq_len: int):\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        i = jnp.arange(0, seq_len)[:, None]\n",
    "        j = jnp.arange(0, d_model, 2)[None, :]\n",
    "        angle = i * 1e-4 ** (j / d_model)\n",
    "\n",
    "        # interleave sin and cos\n",
    "        self.pos_embed = (\n",
    "            jnp.zeros([seq_len, d_model])\n",
    "            .at[:, ::2]\n",
    "            .set(jnp.sin(angle))\n",
    "            .at[:, 1::2]\n",
    "            .set(jnp.cos(angle))\n",
    "        )\n",
    "\n",
    "    def __call__(self, input: jax.Array) -> jax.Array:\n",
    "        assert len(input) <= self.seq_len, f\"{len(input)=} > {self.seq_len=}\"\n",
    "        input *= jnp.sqrt(self.d_model)\n",
    "        return input + jax.lax.stop_gradient(self.pos_embed[: len(input)])\n",
    "\n",
    "\n",
    "class LayerNorm(sk.TreeClass):\n",
    "    \"\"\"Normalize over the last dimension\"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int):\n",
    "        self.in_features = in_features\n",
    "        self.weight = jnp.ones(in_features)\n",
    "        self.bias = jnp.zeros(in_features)\n",
    "\n",
    "    def __call__(self, input: jax.Array):\n",
    "        mean = jnp.mean(input, axis=-1, keepdims=True)\n",
    "        var = jnp.var(input, axis=-1, keepdims=True)\n",
    "        return self.weight * ((input - mean) / jnp.sqrt(var + 1e-5)) + self.bias\n",
    "\n",
    "\n",
    "class MHA(sk.TreeClass):\n",
    "    def __init__(self, d_model: int, num_heads: int, *, key: jax.Array):\n",
    "        self.num_heads = num_heads\n",
    "        assert d_model % num_heads == 0, f\"{d_model=} not divisible by {num_heads=}\"\n",
    "        k1, k2, k3, k4 = jr.split(key, 4)\n",
    "        self.q_projection = sk.nn.Linear(d_model, d_model, bias_init=None, key=k1)\n",
    "        self.k_projection = sk.nn.Linear(d_model, d_model, bias_init=None, key=k2)\n",
    "        self.v_projection = sk.nn.Linear(d_model, d_model, bias_init=None, key=k3)\n",
    "        self.o_projection = sk.nn.Linear(d_model, d_model, bias_init=None, key=k4)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_heads(input: jax.Array, num_heads: int):\n",
    "        return input.reshape(*input.shape[:-1], num_heads, input.shape[-1] // num_heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_heads(input: jax.Array):\n",
    "        return input.reshape(*input.shape[:-2], -1)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        q_input: jax.Array,\n",
    "        k_input: jax.Array,\n",
    "        v_input: jax.Array,\n",
    "        *,\n",
    "        mask: jax.Array | None = None,\n",
    "    ) -> jax.Array:\n",
    "        q_heads = self.split_heads(self.q_projection(q_input), self.num_heads)\n",
    "        k_heads = self.split_heads(self.k_projection(k_input), self.num_heads)\n",
    "        v_heads = self.split_heads(self.v_projection(v_input), self.num_heads)\n",
    "\n",
    "        logits = jnp.einsum(\"...qhd,...khd->...hqk\", q_heads, k_heads)\n",
    "        logits = logits / jnp.sqrt(q_heads.shape[-1])\n",
    "        min_num = jnp.finfo(q_input.dtype).min\n",
    "        logits = logits if mask is None else jnp.where(mask, logits, min_num)\n",
    "\n",
    "        attention_weight = jax.nn.softmax(logits)\n",
    "        attention = jnp.einsum(\"...hqk,...khd->...qhd\", attention_weight, v_heads)\n",
    "        attention = self.merge_heads(attention)\n",
    "\n",
    "        return self.o_projection(attention)\n",
    "\n",
    "\n",
    "class FeedForward(sk.TreeClass):\n",
    "    def __init__(self, d_model: int, *, key: jax.Array):\n",
    "        k1, k2 = jr.split(key)\n",
    "        self.linear1 = sk.nn.Linear(d_model, 4 * d_model, key=k1)\n",
    "        self.linear2 = sk.nn.Linear(4 * d_model, d_model, key=k2)\n",
    "\n",
    "    def __call__(self, input: jax.Array) -> jax.Array:\n",
    "        input = self.linear1(input)\n",
    "        input = jax.nn.relu(input)\n",
    "        input = self.linear2(input)\n",
    "        return input\n",
    "\n",
    "\n",
    "class EncoderBlock(sk.TreeClass):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        drop_rate: float,\n",
    "        *,\n",
    "        key: jax.Array,\n",
    "    ):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "        sa_key, ff_key = jr.split(key)\n",
    "\n",
    "        self.sa = MHA(d_model, num_heads, key=sa_key)\n",
    "        self.sa_dropout = sk.nn.Dropout(drop_rate)\n",
    "        self.sa_norm = LayerNorm(d_model)\n",
    "\n",
    "        self.ff = FeedForward(d_model, key=ff_key)\n",
    "        self.ff_dropout = sk.nn.Dropout(drop_rate)\n",
    "        self.ff_norm = LayerNorm(d_model)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input: jax.Array,\n",
    "        *,\n",
    "        key: jax.Array,\n",
    "        mask: jax.Array | None = None,\n",
    "    ):\n",
    "        sa_dropout_key, ff_dropout_key = jr.split(key)\n",
    "        # add and norm as done in the original paper\n",
    "        sa_out = self.sa(input, input, input, mask=mask)\n",
    "        sa_out = self.sa_dropout(sa_out, key=sa_dropout_key)\n",
    "        input = sa_out + input\n",
    "        input = self.sa_norm(input)\n",
    "\n",
    "        ff_out = self.ff(input)\n",
    "        ff_out = self.ff_dropout(ff_out, key=ff_dropout_key)\n",
    "        input = ff_out + input\n",
    "        input = self.ff_norm(input)\n",
    "\n",
    "        return input\n",
    "\n",
    "\n",
    "class DecoderBlock(sk.TreeClass):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        drop_rate: float,\n",
    "        *,\n",
    "        key: jax.Array,\n",
    "    ):\n",
    "        sa_key, ca_key, ff_key = jr.split(key, 3)\n",
    "\n",
    "        self.sa = MHA(d_model, num_heads, key=sa_key)\n",
    "        self.sa_dropout = sk.nn.Dropout(drop_rate)\n",
    "        self.sa_norm = LayerNorm(d_model)\n",
    "\n",
    "        self.ca = MHA(d_model, num_heads, key=ca_key)\n",
    "        self.ca_dropout = sk.nn.Dropout(drop_rate)\n",
    "        self.ca_norm = LayerNorm(d_model)\n",
    "\n",
    "        self.ff = FeedForward(d_model, key=ff_key)\n",
    "        self.ff_dropout = sk.nn.Dropout(drop_rate)\n",
    "        self.ff_norm = LayerNorm(d_model)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        dec_input: jax.Array,\n",
    "        enc_output: jax.Array,\n",
    "        *,\n",
    "        key: jax.Array,\n",
    "        dec_mask: jax.Array | None = None,\n",
    "        enc_mask: jax.Array | None = None,\n",
    "    ):\n",
    "        sa_dropout_key, ca_dropout_key, ff_dropout_key = jr.split(key, 3)\n",
    "\n",
    "        sa_out = self.sa(dec_input, dec_input, dec_input, mask=dec_mask)\n",
    "        sa_out = self.sa_dropout(sa_out, key=sa_dropout_key)\n",
    "        dec_input = dec_input + sa_out\n",
    "        dec_input = self.sa_norm(dec_input)\n",
    "\n",
    "        ca_out = self.ca(dec_input, enc_output, enc_output, mask=enc_mask)\n",
    "        ca_out = self.ca_dropout(ca_out, key=ca_dropout_key)\n",
    "        dec_input = dec_input + ca_out\n",
    "        dec_input = self.ca_dropout(dec_input, key=ca_dropout_key)\n",
    "\n",
    "        ff_out = self.ff(dec_input)\n",
    "        ff_out = self.ff_dropout(ff_out, key=ff_dropout_key)\n",
    "        dec_input = dec_input + ff_out\n",
    "        dec_input = self.ff_dropout(dec_input, key=ff_dropout_key)\n",
    "\n",
    "        return dec_input\n",
    "\n",
    "\n",
    "class Transformer(sk.TreeClass):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        drop_rate: float,\n",
    "        seq_len: int,\n",
    "        *,\n",
    "        key: jax.Array,\n",
    "    ):\n",
    "        embed_key, encoder_key, decoder_key, fc_key = jr.split(key, 4)\n",
    "\n",
    "        # embedding\n",
    "        enc_embed_key, dec_embed_key = jr.split(embed_key)\n",
    "        self.enc_embed = InputEmbedding(src_vocab_size, d_model, key=enc_embed_key)\n",
    "        self.dec_embed = InputEmbedding(tgt_vocab_size, d_model, key=dec_embed_key)\n",
    "        self.pos_embed = PositionalEmbedding(d_model, seq_len)\n",
    "        self.embed_dropout = sk.nn.Dropout(drop_rate)\n",
    "\n",
    "        # encoder\n",
    "        self.encoders = tuple(\n",
    "            EncoderBlock(d_model, num_heads, drop_rate, key=ki)\n",
    "            for ki in jr.split(encoder_key, num_blocks)\n",
    "        )\n",
    "\n",
    "        # decoder\n",
    "        self.decoders = tuple(\n",
    "            DecoderBlock(d_model, num_heads, drop_rate, key=ki)\n",
    "            for ki in jr.split(decoder_key, num_blocks)\n",
    "        )\n",
    "\n",
    "        # out projection\n",
    "        self.fc = sk.nn.Linear(d_model, tgt_vocab_size, key=fc_key)\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        enc_input: jax.Array,\n",
    "        mask: jax.Array | None = None,\n",
    "        *,\n",
    "        key: jax.Array,\n",
    "    ):\n",
    "        embed_dropout_key, *enc_keys = jr.split(key, len(self.encoders) + 1)\n",
    "        enc_input = self.enc_embed(enc_input)\n",
    "        enc_input = self.pos_embed(enc_input)\n",
    "        enc_input = self.embed_dropout(enc_input, key=embed_dropout_key)\n",
    "\n",
    "        for encoder, key in zip(self.encoders, enc_keys):\n",
    "            enc_input = encoder(enc_input, key=key, mask=mask)\n",
    "        return enc_input\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        enc_output: jax.Array,\n",
    "        dec_input: jax.Array,\n",
    "        enc_mask: jax.Array | None = None,\n",
    "        dec_mask: jax.Array | None = None,\n",
    "        *,\n",
    "        key: jax.Array,\n",
    "    ):\n",
    "        embed_dropout_key, *dec_keys = jr.split(key, len(self.decoders) + 1)\n",
    "\n",
    "        dec_input = self.dec_embed(dec_input)\n",
    "        dec_input = self.pos_embed(dec_input)\n",
    "        dec_input = self.embed_dropout(dec_input, key=embed_dropout_key)\n",
    "\n",
    "        for decoder, key in zip(self.decoders, dec_keys):\n",
    "            dec_input = decoder(\n",
    "                dec_input,\n",
    "                enc_output,\n",
    "                enc_mask=enc_mask,\n",
    "                dec_mask=dec_mask,\n",
    "                key=key,\n",
    "            )\n",
    "\n",
    "        return dec_input\n",
    "\n",
    "    def project(self, input: jax.Array) -> jax.Array:\n",
    "        return self.fc(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_dataset(\n",
    "    config.dataset.name,\n",
    "    name=f\"{config.dataset.src_lang}-{config.dataset.tgt_lang}\",\n",
    "    split=[\"train\", \"test\"],\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer(config: ConfigDict, dataset, lang: str) -> Tokenizer:\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=config.unk_token))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = WordLevelTrainer(\n",
    "        special_tokens=config.special_tokens,\n",
    "        min_frequency=config.min_frequency,\n",
    "    )\n",
    "\n",
    "    def iterator():\n",
    "        for item in dataset:\n",
    "            yield item[\"translation\"][lang]\n",
    "\n",
    "    tokenizer.train_from_iterator(iterator(), trainer=trainer)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "src_tokenizer = build_tokenizer(\n",
    "    config.tokenizer,\n",
    "    dataset=train_dataset,\n",
    "    lang=config.dataset.src_lang,\n",
    ")\n",
    "tgt_tokenizer = build_tokenizer(\n",
    "    config.tokenizer,\n",
    "    dataset=train_dataset,\n",
    "    lang=config.dataset.tgt_lang,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataItem(TypedDict):\n",
    "    # the source language token ids\n",
    "    enc_input: Annotated[jax.Array, \"seq_len\"]\n",
    "    # the target language token ids starting with sos\n",
    "    dec_input: Annotated[jax.Array, \"seq_len\"]\n",
    "    # the target language token ids ending with eos\n",
    "    dec_output: Annotated[jax.Array, \"seq_len\"]\n",
    "    # pad skipping mask for the encoder input\n",
    "    enc_mask: Annotated[jax.Array, \"1, 1, seq_len\"]\n",
    "    # pad skipping + causal mask for the decoder input\n",
    "    dec_mask: Annotated[jax.Array, \"1, seq_len, seq_len\"]\n",
    "    # the source language text\n",
    "    src_text: Annotated[np.ndarray, \"\"]\n",
    "    # the target language text\n",
    "    tgt_text: Annotated[np.ndarray, \"\"]\n",
    "\n",
    "\n",
    "def generate_dataloader(\n",
    "    dataset,\n",
    "    *,\n",
    "    src_tokenizer: Tokenizer,\n",
    "    tgt_tokenizer: Tokenizer,\n",
    "    src_lang: str,\n",
    "    tgt_lang: str,\n",
    "    batch_size: int,\n",
    "    seq_len: int,\n",
    "):\n",
    "    sos_id = np.array([tgt_tokenizer.token_to_id(\"[SOS]\")])\n",
    "    eos_id = np.array([tgt_tokenizer.token_to_id(\"[EOS]\")])\n",
    "    pad_id = np.array([tgt_tokenizer.token_to_id(\"[PAD]\")])\n",
    "\n",
    "    def get_dataitem(index: int) -> DataItem:\n",
    "        src_tgt_item = dataset[index]\n",
    "\n",
    "        # encoder\n",
    "        src_text = src_tgt_item[\"translation\"][src_lang]\n",
    "        # text -> jax array of integer indices\n",
    "        # subtract 2 for sos and eos at the beginning and end of the sequence\n",
    "        src_ids = jnp.array(src_tokenizer.encode(src_text).ids)[: seq_len - 2]\n",
    "        # fill the rest with pad tokens if the sequence is shorter than seq_len\n",
    "        enc_pad = pad_id.repeat(max(0, seq_len - len(src_ids) - 2))\n",
    "        # add sos and eos tokens to the beginning and end of the sequence\n",
    "        enc_input = jnp.concatenate([sos_id, src_ids, eos_id, enc_pad])\n",
    "        # mask out the pad tokens for the encoder input\n",
    "        enc_mask = jnp.where(enc_input == pad_id, False, True)[None, None, :]\n",
    "\n",
    "        # decoder\n",
    "        tgt_text = src_tgt_item[\"translation\"][tgt_lang]\n",
    "        # text -> jax array of integer indices\n",
    "        # subtract 1 for sos at the beginning of the sequence for the decoder input\n",
    "        # or eos at the end of the sequence for the decoder output\n",
    "        tgt_ids = jnp.array(tgt_tokenizer.encode(tgt_text).ids)[: seq_len - 1]\n",
    "        # fill the rest with pad tokens\n",
    "        dec_pad = pad_id.repeat(max(0, seq_len - len(tgt_ids) - 1))\n",
    "        # [<SOS> ...] decoder input\n",
    "        dec_input = jnp.concatenate([sos_id, tgt_ids, dec_pad])\n",
    "        # [... <EOS>] decoder output\n",
    "        dec_output = jnp.concatenate([tgt_ids, eos_id, dec_pad])\n",
    "        # causal mask for the decoder self-attention layer\n",
    "        dec_mask = jnp.tril(jnp.ones([1, seq_len, seq_len])).astype(bool)\n",
    "        # also mask out the pad tokens for the decoder input\n",
    "        dec_mask &= jnp.where(dec_input == pad_id, False, True)[None, None, :]\n",
    "\n",
    "        return dict(\n",
    "            enc_input=enc_input,\n",
    "            dec_input=dec_input,\n",
    "            dec_output=dec_output,\n",
    "            enc_mask=enc_mask,\n",
    "            dec_mask=dec_mask,\n",
    "            src_text=src_text,\n",
    "            tgt_text=tgt_text,\n",
    "        )\n",
    "\n",
    "    def get_batch(indices: list[int]) -> Batched[DataItem]:\n",
    "        batch = [get_dataitem(i) for i in indices]\n",
    "        return jax.tree_map(lambda *args: np.stack(args), *batch)\n",
    "\n",
    "    indices = jnp.arange(len(dataset))\n",
    "\n",
    "    def _dataloader(key: jax.Array):\n",
    "        indices_: list[int] = jax.random.permutation(key, indices).tolist()\n",
    "        for batch_indices in chunked(indices_, batch_size):\n",
    "            yield get_batch(batch_indices)\n",
    "\n",
    "    return _dataloader\n",
    "\n",
    "\n",
    "train_dl = generate_dataloader(\n",
    "    train_dataset,\n",
    "    src_tokenizer=src_tokenizer,\n",
    "    tgt_tokenizer=tgt_tokenizer,\n",
    "    src_lang=config.dataset.src_lang,\n",
    "    tgt_lang=config.dataset.tgt_lang,\n",
    "    batch_size=config.train.batch_size,\n",
    "    seq_len=config.model.seq_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optax.adam(config.optim.lr)\n",
    "train_key = jr.key(config.train.seed)\n",
    "PAD_ID = jnp.array([tgt_tokenizer.token_to_id(\"[PAD]\")])\n",
    "net = Transformer(\n",
    "    src_vocab_size=src_tokenizer.get_vocab_size(),\n",
    "    tgt_vocab_size=tgt_tokenizer.get_vocab_size(),\n",
    "    seq_len=config.model.seq_len,\n",
    "    d_model=config.model.d_model,\n",
    "    num_heads=config.model.num_heads,\n",
    "    num_blocks=config.model.num_blocks,\n",
    "    drop_rate=config.model.drop_rate,\n",
    "    key=jr.key(config.model.seed),\n",
    ")\n",
    "net = sk.tree_mask(net)\n",
    "optim_state = optim.init(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pretrained weights (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# download from google drive\n",
    "# https://drive.google.com/file/d/1h9fV9xlLZFr2XDWzccreelB9QneJNad_/view?usp=sharing\n",
    "\n",
    "# d_model = 512\n",
    "# seq_len = 100\n",
    "# num_heads = 8\n",
    "# num_blocks = 2\n",
    "# drop_rate = 0.1\n",
    "# seed = 0\n",
    "\n",
    "treedef = jax.tree_util.tree_structure(net)\n",
    "with open(\"transformer_weights.pickle\", \"rb\") as file:\n",
    "    leaves = pickle.load(file)\n",
    "net = jax.tree_util.tree_unflatten(treedef, leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(logits, idx):\n",
    "    logprop = jax.nn.log_softmax(logits)\n",
    "    logprop = jnp.take_along_axis(logprop, idx[..., None], axis=-1)[..., 0]\n",
    "    # ignore pad tokens\n",
    "    logprop = jnp.where(idx == PAD_ID, 0.0, logprop)\n",
    "    return jnp.sum(-logprop)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    net: Transformer,\n",
    "    optim_state: optax.OptState,\n",
    "    enc_input: Batched[Annotated[jax.Array, \"seq_len\"]],\n",
    "    dec_input: Batched[Annotated[jax.Array, \"seq_len\"]],\n",
    "    dec_output: Batched[Annotated[jax.Array, \"seq_len\"]],\n",
    "    enc_mask: Batched[Annotated[jax.Array, \"1,1,seq_len\"]],\n",
    "    dec_mask: Batched[Annotated[jax.Array, \"1,seq_len,seq_len\"]],\n",
    "    key: jax.Array,\n",
    "):\n",
    "    k1, k2 = jr.split(key)\n",
    "\n",
    "    @ft.partial(jax.grad, has_aux=True)\n",
    "    def loss_func(net: Transformer):\n",
    "        net = sk.tree_unmask(net)\n",
    "        encode = jax.vmap(ft.partial(net.encode, key=k1))\n",
    "        decode = jax.vmap(ft.partial(net.decode, key=k2))\n",
    "        project = jax.vmap(net.project)\n",
    "        enc_pred: Batched[jax.Array] = encode(enc_input, enc_mask)\n",
    "        dec_pred: Batched[jax.Array] = decode(enc_pred, dec_input, enc_mask, dec_mask)\n",
    "        logits: Batched[jax.Array] = project(dec_pred)\n",
    "        loss: Batched[jax.Array] = jax.vmap(softmax_cross_entropy)(logits, dec_output)\n",
    "        # similar to torch cross_entropy, with ignore_index=PAD_ID and reduction=\"mean\"\n",
    "        loss = jnp.sum(loss) / jnp.sum(jnp.where(dec_output == PAD_ID, 0.0, 1.0))\n",
    "        return loss, loss\n",
    "\n",
    "    grads, loss = loss_func(net)\n",
    "    updates, optim_state = optim.update(grads, optim_state)\n",
    "    net = optax.apply_updates(net, updates)\n",
    "    return net, optim_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_key = jr.key(config.train.seed)\n",
    "batches = len(train_dataset) // config.train.batch_size\n",
    "for i in (pbar := tqdm(range(config.train.epochs))):\n",
    "    train_key = jr.fold_in(train_key, i)\n",
    "\n",
    "    dl_key, train_step_key = jr.split(train_key)\n",
    "\n",
    "    epoch_loss = []\n",
    "    for j, batch in tqdm(enumerate(train_dl(key=dl_key)), total=batches):\n",
    "        train_step_key = jr.fold_in(train_step_key, j)\n",
    "\n",
    "        net, optim_state, loss = train_step(\n",
    "            net,\n",
    "            optim_state,\n",
    "            batch[\"enc_input\"],\n",
    "            batch[\"dec_input\"],\n",
    "            batch[\"dec_output\"],\n",
    "            batch[\"enc_mask\"],\n",
    "            batch[\"dec_mask\"],\n",
    "            train_step_key,\n",
    "        )\n",
    "        epoch_loss += [loss]\n",
    "        pbar.set_description(f\"loss: {loss:.4f}\")\n",
    "\n",
    "    pbar.set_description(f\"loss: {np.mean(epoch_loss):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    text: str,\n",
    "    *,\n",
    "    net: Transformer,\n",
    "    src_tokenizer,\n",
    "    tgt_tokenizer,\n",
    "    max_len: int,\n",
    "    key: jax.Array,\n",
    ") -> str:\n",
    "    sos_id = np.array([tgt_tokenizer.token_to_id(\"[SOS]\")])\n",
    "    eos_id = np.array([tgt_tokenizer.token_to_id(\"[EOS]\")])\n",
    "    pad_id = np.array([tgt_tokenizer.token_to_id(\"[PAD]\")])\n",
    "\n",
    "    # text -> jax array of integer indices\n",
    "    # subtract 2 for sos and eos at the beginning and end of the sequence\n",
    "    src_ids = jnp.array(src_tokenizer.encode(text).ids)[: max_len - 2]\n",
    "    # fill the rest with pad tokens if the sequence is shorter than seq_len\n",
    "    enc_pad = pad_id.repeat(max(0, max_len - len(src_ids) - 2))\n",
    "    # add sos and eos tokens to the beginning and end of the sequence\n",
    "    enc_input = jnp.concatenate([sos_id, src_ids, eos_id, enc_pad])\n",
    "\n",
    "    # 1, 1, seq_len\n",
    "    enc_mask = jnp.where(enc_input == pad_id, False, True)[None, None, :]\n",
    "    enc_key, dec_key = jr.split(key)\n",
    "    enc_output = net.encode(enc_input, enc_mask, key=enc_key)\n",
    "    dec_input = jnp.array([tgt_tokenizer.token_to_id(\"[SOS]\")])\n",
    "\n",
    "    while True:\n",
    "        dec_key = jr.fold_in(dec_key, len(dec_input))\n",
    "        # causal mask\n",
    "        dec_mask = jnp.tril(jnp.ones([1, len(dec_input), len(dec_input)])).astype(bool)\n",
    "        # skip pad tokens\n",
    "        dec_mask &= jnp.where(dec_input == pad_id, False, True)[None, None, :]\n",
    "\n",
    "        # seq_len, d_model\n",
    "        dec_output = net.decode(enc_output, dec_input, enc_mask, dec_mask, key=dec_key)\n",
    "        logits = net.project(dec_output)[..., -1, :]\n",
    "        next_token = jnp.argmax(logits, axis=-1)[None]\n",
    "\n",
    "        if next_token == eos_id:\n",
    "            # reached end of sequence\n",
    "            break\n",
    "\n",
    "        dec_input = jnp.concatenate([dec_input, next_token], axis=-1)\n",
    "\n",
    "    return tgt_tokenizer.decode(dec_input.tolist())\n",
    "\n",
    "\n",
    "translate_from_arabic_to_english = ft.partial(\n",
    "    predict,\n",
    "    net=sk.tree_eval(sk.tree_unmask(net)),\n",
    "    src_tokenizer=src_tokenizer,\n",
    "    tgt_tokenizer=tgt_tokenizer,\n",
    "    max_len=config.model.seq_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input arabic: حسناً ، تبعاً للتقرير هناك ثلاث عبوات ماء مفقودة\n",
      "true english: Well, according to the report, there were three water bottles missing.\n",
      "pred english: Well , according to the report is three of the missing water .\n",
      "\n",
      "input arabic: هل سترجع للعيش معنا هنا\n",
      "true english: (Maya) So Are You Gonna Move Back In,\n",
      "pred english: You ' re gonna get him in here .\n",
      "\n",
      "input arabic: مما أنت خائف؟\n",
      "true english: What are you scared of?\n",
      "pred english: Which you ' re afraid ?\n",
      "\n",
      "input arabic: هيّا يا مهدّئات العضلات.\n",
      "true english: Come on, muscle relaxers.\n",
      "pred english: Come on , .\n",
      "\n",
      "input arabic: كما انك طردتها.\n",
      "true english: As you rejected it.\n",
      "pred english: As you ' re a .\n",
      "\n",
      "input arabic: ثم أنه ليس له.\n",
      "true english: Then it's not him.\n",
      "pred english: Then he ' s not for him .\n",
      "\n",
      "input arabic: وينبغي ألا تقدم التصويبات إلا للنص باللغات الأصلية.\n",
      "true english: Corrections should be submitted to the original languages only.\n",
      "pred english: Corrections should not be submitted only in the original languages .\n",
      "\n",
      "input arabic: ...لديّ الكثير، لذا إذا أردتي أن تُـحضِري أصدقاء، إفعلي هذا\n",
      "true english: I have more, so if you want to bring friends, do that..\n",
      "pred english: I have a lot , so if you want to get friends , do it .\n",
      "\n",
      "input arabic: لا أُريد أن أهرب.\n",
      "true english: We can't stay here! I won't run!\n",
      "pred english: I don ' t want to run away .\n",
      "\n",
      "input arabic: وكثيراً ما يكون ذلك التعاون في شكل وضع إطار مؤسسي لمتابعة مسائل الاستثمار وتحديد الأطر الزمنية للشروع في مفاوضات مستقبلية بشأن تحرير و/أو حماية الاستثمار.\n",
      "true english: However, some agreements establish only a framework for cooperation between the contracting parties.\n",
      "pred english: This cooperation is often recognized in the form of institutional and implementation of investment issues and identify the appropriate frameworks for the liberalization of trade and / or protecting investment .\n",
      "\n",
      "input arabic: الرجل الروسي الذي يتحدث بتسعة لغات مختلفة.\n",
      "true english: The Russian guy... speaks like, nine different languages. Uh, uh, Danny's friend.\n",
      "pred english: The Russian man who speaks to various languages .\n",
      "\n",
      "input arabic: فى رايى لا يوجد اسوأ من شرطى جبان ملعون.\n",
      "true english: There's nothin' worse in my book than a goddamn yellow cop.\n",
      "pred english: In the , there ' s no worse than a loser .\n",
      "\n",
      "input arabic: تتذكرين ليلة أمس عندما كنتِ\n",
      "true english: Remember last night when you were all,\n",
      "pred english: Remember last night , when you were .\n",
      "\n",
      "input arabic: بحقك يا (غيدجيت) سنخرج هناك بدون حزام؟\n",
      "true english: Come on, Gidget. We go out there without a leash...\n",
      "pred english: Come on , , we ' ll get out there without a belt ?\n",
      "\n",
      "input arabic: \"إنني واثقاً بحسن نية قدومكم ولقد قبلتُ القبطان (فلينت) ضيفاً لي،\n",
      "true english: \"I trusted the good faith of your arrival and I accepted Captain Flint as my guest in the same spirit.\n",
      "pred english: I ' m sure good faith in the , and I ' m gonna get the captain Flint .\n",
      "\n",
      "input arabic: خلـط التنـدّب أعضـاءه ببعضهـا\n",
      "true english: It's worse than I thought.\n",
      "pred english: .\n",
      "\n",
      "input arabic: الفائز يحصل على الفتاة\n",
      "true english: Winner gets the girl.\n",
      "pred english: The gets the girl .\n",
      "\n",
      "input arabic: هل يمكنك معرفة ما إذا كان أي شيء يحدث\n",
      "true english: Can you tell if anything's happening,\n",
      "pred english: Can you know if anything happens ?\n",
      "\n",
      "input arabic: ونأمل ونتوقع تلقي رد واضح من الحكومة الإسرائيلية على الاقتراح في القريب العاجل.\n",
      "true english: We hope for and expect a clear answer on the proposal from the Israeli Government very soon.\n",
      "pred english: We hope that the Israeli Government will receive a clear response to the proposal in the near conclusion of the Israeli Government .\n",
      "\n",
      "input arabic: بعدها علينا حجزك وإلخ إلخ.\n",
      "true english: Then we gotta book you, blah, blah.\n",
      "pred english: Then we have to get a .\n",
      "\n",
      "input arabic: لم يتناقش أحد معي عندما أحضرتموه من البداية\n",
      "true english: - Nobody asked me when we brought him.\n",
      "pred english: Nobody ' s been involved with me when you were running out of the beginning .\n",
      "\n",
      "input arabic: توقف النزيف\n",
      "true english: The bleeding stopped.\n",
      "pred english: Stop the bleeding .\n",
      "\n",
      "input arabic: في حياتك\n",
      "true english: Of your whole life.\n",
      "pred english: In your life .\n",
      "\n",
      "input arabic: ربما الجاني مبتور الساق\n",
      "true english: Maybe the unsub is an amputee.\n",
      "pred english: Maybe the ' s leg .\n",
      "\n",
      "input arabic: إذن، ماكغي\n",
      "true english: So McGee,\n",
      "pred english: So , McGee .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(0, 50, 2):\n",
    "    text_ar = test_dataset[index][\"translation\"][\"ar\"]\n",
    "    text_en = test_dataset[index][\"translation\"][\"en\"]\n",
    "    text_en_pred = translate_from_arabic_to_english(text_ar, key=jr.key(0))\n",
    "\n",
    "    print(\n",
    "        f\"input arabic: {text_ar}\\n\"\n",
    "        f\"true english: {text_en}\\n\"\n",
    "        f\"pred english: {text_en_pred}\\n\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
